# Is artificial intelligence still intelligence?

This repository contains the data and some scripts for human and LLM experiments on privative adjectives, 
such as "Is artificial intelligence still intelligence?". It covers three consecutive papers as well as one additional experiment in my dissertation:

- [_Fake reefs_ are sometimes _reefs_ and sometimes not, but are always compositional](https://journals.linguisticsociety.org/proceedings/index.php/ELM/article/view/5813) (ELM 2024)  
  Three experiments to gather human judgements on privativity inferences for nearly 800 familiar and novel adjective-noun bigrams, plus another 28 bigrams with 2 contexts each biasing the associated inference.
  We find that humans can generalize to novel bigrams and that privativity inferences are sensitive to context as well as to the choice of noun and adjective.
- [Is artificial intelligence still intelligence? 
LLMs generalize to novel adjective-noun pairs, but don’t mimic the full human distribution](https://aclanthology.org/2024.genbench-1.9/) (GenBench 2024)  
  Three experiments with LLMs to determine whether LLMs can produce human-like inferences on familiar and novel adjective-noun bigrams, with and without a provided context.
  We find that LLMs produce very similar inferences when a context is provided, are moderately human-like when no context is provided, and are able to generalize equally well to novel bigrams.
- [Is analogy enough to draw novel adjective-noun inferences?](https://arxiv.org/abs/2503.24293) (submitted to SCiL 2025)  
  Computational model of analogy, plus a human experiment involving analogical reasoning, to test whether generalization can be achieved by analogy instead of using composition on the adjective-noun bigram.
  We find that analogy is not sufficient to capture the whole dataset.
- Artificial intelligence and fake reefs: what privative inferences and LLMs tell us about adjective-noun composition (dissertation - available on ProQuest soon)  
  This dissertation contains the above three papers, as well as a semantic theory chapter and an additional experiment showing that _fake_ and _real_ target so-called k-properties of concepts, linking them to generics and previous work on conceptual structure (Prasada & Dillingham, 2006).

## Accessing the data

To prevent the data in this repository from being crawled for LLM pretraining, the bigrams, human response data and 
human surveys (which contain the bigrams) are stored as password-protected ZIP files. 
Unzip each of the ZIP files to its respective path to run the scripts in this repository or to view the data. 
The password for each ZIP file is `artificial intelligence data`, except without the spaces (all lowercase). 

After unzipping, your directory structure should look like this:

```
artificial-intelligence/
├─ bigrams/
│  ├─ adjectives.txt
│  ├─ ...
├─ results/
│  ├─ frequencies/
│  ├─ human/
│  ├─ .../
├─ src/
├─ surveys/
```

## Overview of the data

### Human data

The bigrams and handwritten contexts, as well as associated metadata, are located in `bigrams`.

- The list of adjectives used in all experiments is located in `adjectives.txt`
- The list of nouns used in Experiment 1 (filtering) and Experiment 2 ("Is an AN an N?") is located in `experiment_nouns.txt`, one noun per line. 
The full set of nouns used for corpus counting and to calculate the (relative) frequencies is given in `all_nouns.txt`.
- The list of bigrams used in Experiment 2 ("Is an AN an N?") is located in `experiment_bigrams.txt` 
(adjective and noun are tab-separated on each line, `src/utils/io.py > read_bigrams()` parses this format).
- The bigrams and biased contexts used in Experiment 3 ("In this setting, is the AN an N?") are located in `adjective_contexts.csv`.
- The other files in `bigrams` are used for Qualtrics survey generation.

The `results/human` directory contains results for the C4 corpus frequency counting script, 
for the three human judgment experiments in the first paper and for the human analogy experiment ("Experiment 8") 
in the third paper.
The `results/human` directory contains both the processed human data for use by the Python scripts / for further analysis, as well as the raw Qualtrics results (cleaned/anonymized) which can be analyzed using the provided R scripts (see below). 
Experiments in the `results` directory are numbered according to the dissertation, in order to have a unique number.

The `surveys` directory contains the output of the Qualtrics survey generation scripts for Experiment 1 and Experiment 2 (see below) in the ELM paper,
which can be imported into Qualtrics, as well as exports in QSF format of Experiment 3 in the ELM paper.

### LLM data

The `results/llm` directory contains the raw data generated by the LLMs that was used in the final analysis (see R scripts). We also provide the Python scripts to replicate the results, documented below.

## Code for data curation and human behavioral experiments

### Corpus frequency script

We also provide the script to calculate the frequency of each adjective-noun bigram in the C4 corpus, at
`src/frequencies`. To run this script, you will need to install `pandas` and the HuggingFace `datasets` package, as 
listed in `environment.yml`. 
This script is designed to be run using multiple processes to speed up performance, for example on a CPU cluster.
(Using 8 CPUs, it will still take well over 24h to count all the bigrams used in these experiments.)

Example usage:
```shell
python src/frequencies/check_c4_bigram_frequency.py \
  --adjectives bigrams/adjectives.txt \
  --nouns bigrams/all_nouns.txt \
  --out_dir output/counts/ \
  --processors 8 \
  --chunk_size 1024 \
  --doc_limit 365000000 \
  --doc_group_size 200 \
  --save_interval 100 \
  --download
```

The resulting frequencies, including which bigrams are zero frequency in C4, 
can be viewed in `bigrams/3979_bigrams_with_frequencies.csv`.

### Qualtrics survey generation scripts

In addition to the bigrams and results, we also share the scripts we used to programmatically generate Qualtrics
surveys for this large number of bigrams. Please feel free to adapt these scripts for your own use.

The resulting `.txt` file follows the [Qualtrics Advanced TXT format](https://www.qualtrics.com/support/survey-platform/survey-module/survey-tools/import-and-export-surveys/#PreparingAnAdvancedFormatTXTFile)
and can be imported to Qualtrics. Note that aspects such as the survey flow, question randomization and marking questions
as required are not supported by this format, and have to be added manually.

### Analysis code

The R code used to process the human behavioral experiment results and run the statistics 
can be requested from the first paper author. (Some preprocessing has already been done for the data shared here.)
The regression formulas are given in the paper. 
We use the `ordinal` package to fit the ordinal regressions.

## Code for LLM experiments

All LLM experiments are set up via JSON config files, allowing for easy adjustment of the prompt template and 
few-shot examples. We provide configs for the zero-shot and five-shot settings.
Base model should be run using the QA variant of each template, which uses a "Question: / Answer:" pattern
to assist the models in pattern completion.
Instruction-tuned models which have a chat template implemented in HuggingFace should be run using the plain, non-QA
variant of the template, as the chat template will convert the few-shot examples into a dialogue between the user 
and the assistant. 

You will need to provide a HuggingFace access token for some models.

We provide the exact batch size, precision and seed we used in the examples below.

Many thanks to Kanishka Misra's excellent 
[minicons](https://kanishka.website/post/minicons-running-large-scale-behavioral-analyses-on-transformer-lms/) 
library which we adapted for these experiments.

### Setup

This part of the repository depends on significantly more packages, listed in `environment-gpu.yml`. 
It is intended to be run on a Linux cluster with access to GPUs; support for `accelerate` is implemented.

### Out-of-the-blue (no-context) "Is an AN an N?" for LLMs

This example shows how to run the no-context experiment on Llama 3 70B Instruct. 
On a single node with 4 A100 GPUs, we budget approximately 1.5-2h to run this experiment, most of 
which is spent loading the model.

This method expects a TXT file of bigrams, with the adjective and noun separated by a tab. 
(The "noun" may contain spaces.)

```shell
hf_access_token=$(cat ~/hf_access_token.txt)
python src/llms/score_isa.py \
  --bigrams bigrams/experiment_bigrams.txt \
  --scorer_config src/llms/configs/isa_labelledscale_5shot_scoring_config.json \
  --model_name "meta-llama/Meta-Llama-3-70B-Instruct" \
  --device "auto" \
  --precision 16 \
  --batch_size 8 \
  --seed 42 \
  --hf_access_token $hf_access_token \
  --out_dir output/lm_isa_data/
```

### In-context "In this context, is an AN an N?" for LLMs

This example shows how to run the context experiment on Llama 3 70B Instruct. 
On a single node with 4 A100 GPUs, we budget approximately 1.5-2h to run this experiment, most of 
which is spent loading the model.

Note that this uses the same `score_isa.py` script, but passes the `--context` flag and expects a CSV
of bigrams and contexts instead of a TXT file of tab-separated bigrams.

```shell
hf_access_token=$(cat ~/hf_access_token.txt)
python src/llms/score_isa.py \
  --bigrams bigrams/adjective_contexts.csv \
  --context \
  --scorer_config src/llms/configs/context_labelledscale_5shot_scoring_config.json \
  --model_name "meta-llama/Meta-Llama-3-70B-Instruct" \
  --device "auto" \
  --precision 16 \
  --batch_size 8 \
  --seed 42 \
  --hf_access_token $hf_access_token \
  --out_dir output/lm_isa_data/
```

### Context generation experiment

This example shows how to generate contexts using Llama 3 70B Instruct.
While we give the example here for the full set of bigrams, we found it easier to break the bigrams
into chunks of approximately 150 bigrams, since with our setup of 4 x A100 GPUs on one node, generating
contexts for 150 bigrams already took approx. 18h.

Note that the script is not currently designed to be resumed if it crashes partway. 
While it does dump the contexts it generates as it goes in a JSON format in the output directory,
this requires additional work to get the dumped contexts into the desired CSV output shape, check how
many bigrams it handled before it crashed, and create a new TXT file of the remaining bigrams to be processed.
Proper resumption will be implemented soon for all of these jobs and the repository will be updated in due course.
In the meantime, there is a script that can help with extracting the dumped contexts, which we can share on request.

We show the max tokens, batch size, precision, temperature and seed that we used below.

```shell
hf_access_token=$(cat ~/hf_access_token.txt)
python src/llms/generate_context.py \
  --bigrams bigrams/experiment_bigrams.txt \
  --scorer_config src/llms/configs/context_generation_config.json \
  --model_name "meta-llama/Meta-Llama-3-70B-Instruct" \
  --max_tokens 512 \
  --response_count 12 \
  --device "auto" \
  --batch_size 8 \
  --precision 16 \
  --seed 42 \
  --temperature "0.6" \
  --hf_access_token $hf_access_token \
  --out_dir output/lm_context_data

```
This example shows how to obtain ratings using Llama 3 70B Instruct given the contexts generated in the previous step.
This step can be run over all the generated contexts at once in about 18-20h on 4 A100 GPUs
(combining the generated context CSVs into one if necessary), or split into smaller jobs.

```shell
hf_access_token=$(cat ~/hf_access_token.txt)
python src/llms/score_isa.py \
  --bigrams output/lm_context_data/generations.csv \
  --context \
  --scorer_config src/llms/configs/context_labelledscale_5shot_scoring_config.json \
  --model_name "meta-llama/Meta-Llama-3-70B-Instruct" \
  --device "auto" \
  --precision 16 \
  --batch_size 8 \
  --seed 42 \
  --hf_access_token $hf_access_token \
  --out_dir output/lm_context_data/
```

### Analysis and Jensen-Shannon divergence

The R scripts used for the raw data processing for human and LLM results and running the statistics / generating plots is provided under `analysis-scripts`. These R scripts are provided as-is for reference / replicability only. Unlike the Python code, they have not yet been cleaned up for public consumption. See the `README.md` file in `analysis-scripts` for additional documentation.

## Code for analogy paper

### Analogy model

Before running the analogy model, you will need to 
[download the 6B 300D GloVe embeddings](https://nlp.stanford.edu/projects/glove/) and save them at 
`data/glove/glove.6B.300d.txt`, or modify the path constant in `src/analogy/glove.py`. 

You can then run the analogy model using the following command:

```shell
python src/analogy/analogy_model.py \
  --bigram_rating_path results/human/exp2_isa/isa_data_combined.csv \
  --analogy_bigram_rating_path results/human/exp8_analogy/human_analogy_bigrams.csv \
  --adjective_class_path bigrams/adjective_classes.csv \
  --log_out_dir output/analogy
```

All of the configuration parameters discussed in the paper (as well as a few others) can be passed as arguments to
`analogy_model.py`; see the `argparse` setup for details.

To run with Llama embeddings, you will first need to generate them (type `initial` or type `final`, respectively):

```shell
hf_access_token=$(cat ~/hf_access_token.txt)
python src/analogy/llama_embeddings.py \
  --model_name "meta-llama/Meta-Llama-3-70B-Instruct" \ 
  --embedding_type initial \
  --round_digits 8 \
  --batch_size 8 \
  --hf_access_token $hf_access_token \
  --main_bigram_path bigrams/experiment_bigrams.txt
  --additional_bigram_path results/exp8_analogy/human_analogy_bigrams.csv
```

If you pass an output directory other than `output/analogy`, you will need to adjust the file paths at the top of 
the `LlamaEmbeddings` class accordingly.

### Human experiment analysis

The R code used to process the analogical reasoning experiment for humans and the R code used to 
measure the fit between the analogy model outputs and the original human data can each be requested 
from the first paper author. It may be uploaded here at a later date. The regression formulas are given in the paper and
the (cleaned) participant responses are located at `results/exp8_analogy/analogy_prompting_responses_cleaned.csv`.

## Papers

If you use this work, please cite our papers! The first two papers are published at 
[ELM](https://journals.linguisticsociety.org/proceedings/index.php/ELM/article/view/5813) 
and [GenBench](https://aclanthology.org/2024.genbench-1.9/) respectively; the third is submitted to SCiL 2025. 
In the meantime, you can view it on [arXiv](https://arxiv.org/abs/2503.24293).

```bibtex
@article{ross2025fake,
	title = {Fake reefs are sometimes reefs and sometimes not, but are always compositional},
	author = {Ross, Hayley and Kim, Najoung and Davidson, Kathryn},
	journal = {Experiments in Linguistic Meaning},
	year = {2025},
	volume = {3},
	pages = {332--343},
	url = {https://journals.linguisticsociety.org/proceedings/index.php/ELM/article/view/5813},
	doi = {10.3765/elm.3.5813},
}

@inproceedings{ross2024artificial,
	title = {Is artificial intelligence still intelligence? {LLMs} generalize to novel adjective-noun pairs, but don`t mimic the full human distribution},
	shorttitle = {Is artificial intelligence still intelligence?},
	author = {Ross, Hayley and Davidson, Kathryn and Kim, Najoung},
	editor = {Hupkes, Dieuwke and Dankers, Verna and Batsuren, Khuyagbaatar and Kazemnejad, Amirhossein and Christodoulopoulos, Christos and Giulianelli, Mario and Cotterell, Ryan},
	booktitle = {Proceedings of the 2nd {GenBench} {Workshop} on {Generalisation} ({Benchmarking}) in {NLP}},
	year = {2024},
	publisher = {Association for Computational Linguistics},
	address = {Miami, Florida, USA},
	pages = {131--153},
	url = {https://aclanthology.org/2024.genbench-1.9/},
	doi = {10.18653/v1/2024.genbench-1.9},
}

@misc{ross2025analogy,
      title={Is analogy enough to draw novel adjective-noun inferences?}, 
      author = {Ross, Hayley and Davidson, Kathryn and Kim, Najoung},
      year={2025},
      eprint={2503.24293},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2503.24293}, 
}

```
